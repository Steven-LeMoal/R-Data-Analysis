---
title: "CH3 : Logistic Regression"
subtitle: " TP4"
author: LE MOAL STEVEN
output: 
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


>In this PW we are going to analyse the `Social_Network_Ads` dataset. This dataset contains informations of users of a social network. The social network has several business clients and its business clients put ads on the social network for marketing compaigns purposes. For this dataset, a company has put ads for one of its new products and the social network gathered some informations about wich users responded positively to the ad by buying the product and those who responded negatively by not buying the product.


# Question 1 

Import the `Social_Network_Ads` dataset into `R`

```{r}
dataset = read.csv("/Users/steven/Documents/ESILV A4/Machine Learning/TP4/Social_Network_Ads.csv")
```

# Question 2

Describe the dataset (you can use `str()` and `summary()` functions).

```{r}
str(dataset)
```

```{r}
summary(dataset)
```


>We will consider the variables Age and EstimatedSalary as input variables (features) to see the correlations between them and the decision of the user to buy (or not) the product

# Question 3

Now we are going to split the dataset into training set and test set. Last week we did it manually. From now on split it randomly with caTools packages

```{r}
library(caTools) # install it first in the console
set.seed(123) # we use this function with the same number to randomly generate the same values

split = sample.split(dataset$Purchased, SplitRatio = 0.75)

#training_data = dataset[split,]
#testing_data = dataset[-split,]
training_data = subset(dataset, split == TRUE)
testing_data = subset(dataset, split == FALSE)
str(training_data)
```

# Question 4

Scale the input variables in both training set and test set.

```{r}
training_data[, 3:4] = scale(training_data[, 3:4])
testing_data[, 3:4] = scale(testing_data[, 3:4])
summary(training_data)
```

# Question 5

Fit a simple logistic regression model of `Purchased` in function of `Age`

```{r}
model1 = glm(Purchased~Age, data = training_data,family=binomial)
summary(model1)

```

# Question 6

In the argument `family` of the function `glm` we chose `binomial`. Why ?


```{r}
#In order to tell R to do a logistic regression, rather then some other type of generalised linear regression
#We chose binomial because the outcome is between 0 and 1.
```

# Question 7

What is the equation of the obtained model in the question 1.5 ?


```{r}
#Y = 1 + 1/exp(B1 X + B0) (using summary(model1))
#Y : Purchased,  X : Age,  B1 = 1.9913 ,  B0 = -0.9299 
```

# Question 8

Is the feature `Age` significant?

```{r}
#If abs(coefficient) > 0, here B1 = 1.9913 and p-value 2e-16 (< 0.05) : mean age is significant
```


The `AIC` is the `Akaike Information Criterion`. You will use this while comparing multiple models. The model with lower value of `AIC` is better. Suppose that we have a statistical model of some data. Let $L$ be the maximum value of the likelihood function for the model; let `k` be the number of estimated parameters in the model. Then the `AIC` value of the model is the following.

$AIC=2k−2ln(L)$

where

* $L$ = the maximized value of the likelihood function of the model $M$, i.e. $L=p(x|β,M)$, where $β$ are the parameter values that maximize the likelihood function.
* x= the observed data.
* k= the number of free parameters to be estimated. If the model under consideration is a linear regression, k is the number of regressors, including the intercept.


# Question 9

What is the value of AIC of the model

```{r}
#(using summary(model1))
#AIC: 256.11
```

# Question 10

Plot `Purchased` in function of `Age` and add the curve of the obtained logistic regression model.

(Hints: First plot the point, then use the `curve()` function with option `add=TRUE` to add the curve to the plot. The argument `type` of the function `predit()` must be `reponse`)



```{r}
plot(training_data$Age, training_data$Purchased,
xlab = " Age",
ylab = "Purchased",
col = "red",
pch = 20)
curve(predict(model1, data.frame(Age=x), type="response"), add=TRUE)
```

# Question 11

Now let us take another feature into account in the model. Fit a logistic regression model of `purchasing` the product in function of the `age` of the user and its `salary`.


```{r}
model2 = glm(Purchased~Age+EstimatedSalary, data = training_data,family=binomial)
summary(model2)
```

# Question 12


Are the predictors significant? Did the model get better by adding the estimated salary?

```{r}
#model equation : Y = 1 + 1/exp(b0 + b1 X1 + b2 X2)

#Y : Purchased,  X1 = Age, X2 = Estimated salary; b0 = -1.1923,  b1 = 2.6324, b2 = 1.3947

#Age p-value : 2.83e-14
#Estimated salary p-value : 2.03e-09

#Both b1 and b2 are (> 0) and in particular (> 1), both p-value are small (< 0.05) : predictors significant

#Yes, the model get better since AIC: 205.78 (< 256.11)
```

# Question 13

Did the model get better by adding the estimated salary?

```{r}
#Yes, the model get better since AIC: 205.78 (< 256.11)
```

# Question 14

On the test set, predict the probability of purchasing the product by the users using the obtained model


```{r}
predicted_values <- predict(
  model2, 
  data.frame(Age = testing_data$Age, EstimatedSalary = testing_data$EstimatedSalary), 
  type = "response")

predicted_values
```

# Question 15

Take a look on your predicted values for the variable Purchased. We predicted the probability that the user will purchase the product right? Now in order to compare your results with the real answers, transform the predicted values to 0 or 1 (1 if >0.5).


```{r}
predicted_values = ifelse(predicted_values > 0.5, 1, 0)
predicted_values
```

# Question 16

Now to evaluate the predictions, compute the confusion matrix. What do you obtain ?

```{r}
cm = table(predicted_values, testing_data$Purchased)
cm
#We obtain the number of True Positive, True Negative, False Positive, False Negative
```

or 

```{r}
library(caret)
confusionMatrix(as.factor(predicted_values), as.factor(testing_data$Purchased), positive = "1")
```

# Question 17

Calculate the accuracy, specificity, sensitivity and the precision of the model

```{r}
#we call look above or..
accuracy <- (cm[1] + cm[4]) / sum(cm[1:4])
#True negative and positive predicted on all
accuracy

specificity <- cm[1] / (cm[1] + cm[2])
#True negative predicted on negative overall
specificity
  
sensitivity <- cm[4] / (cm[4] + cm[3])
#True positive predicted on positive overall
sensitivity

precision <- cm[4] / (cm[4] + cm[2])
#True positive predicted on True positive and False negative
precision
```




# Question 18

Plot the ROC curve and calculate AUC value

```{r}
#install.packages("ROCR")
library(ROCR)
```


```{r}
prob <- predict(
  model2, 
  testing_data[c(3,4)], 
  type = "response") 
  
score <- prediction(prob,testing_data[,5])

plot(performance(score,"tpr","fpr"),col="green")
abline(0,1,lty=8,col="red")
```

# Question 19

Compare the AUC of the two models you fitted (one with only age and one with age and estimated salary) and plot their ROC curves in the same figure.



```{r}
#model with age only
prob2 <- predict(
  model1, 
  testing_data[c(3)], 
  type = "response") 
score2 <- prediction(prob2, testing_data$Purchased)
score2
```
```{r}
#model with age

performance(score2,"auc")@y.values[[1]]

#model with age and estimated salary
#look previous question for code

performance(score,"auc")@y.values[[1]]
```


```{r}

#AUC of the second model is higher (age and estimated salary : 0.91) 
#than the first model (age only : 0.88)

#but if we look at the plot under,
#we can see that we need to make trade-off between precision and recall

#the model (with age and estimated salary) 
#is better if we accept a higher FPR : (Tpr : 0.92, Fpr : 0.17)

#for the model with age 
#is better if we accept a lesser TPR: (Tpr : 0.81, Fpr : 0.12)

plot(performance(score2,"tpr","fpr"),col="blue")
plot(performance(score,"tpr","fpr"),col="green",add="T")
abline(0,1,lty=8,col="red")
```

